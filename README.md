# Криптонит: Классификация эмоций в текстах
Задача для хакатона (проектного практикума) МФТИ ПУСК «Науки о данных», декабрь 2024.
### Задача: 
Обучить языковую модель для классификации эмоций в текстах на русском языке. Количество эмоций - 7, при этом один текст может содержать не одну эмоцию, а несколько (multiclass multilabel classification).

## Состав команды и роли
- Вяткин Роман: Капитан, Scrum master, Data Scientist
- Новиков Валентин: ML Инженер
- Назаров Михаил: ML Инженер
- Яськова Марина: ML Инженер
- Ихматуллаев Даврон: ML Инженер
- Заславская Вероника: Data Scientist, Data Engineer
- Косачев Дмитрий: MLOps

## Актуальное состояние проекта
Подготовлен ноутбук с промежуточным решением задачи: [ссылка](./emotion_classification_datasorceres.ipynb).
* Ноутбук подготовлен на основе baseline-решения, предоставленного заказчиком.
* На данный момент наш лучший результат по метрике weighted f1-score на тестовой выборке — **0.63**. Метрика рассчитывается тестами [Kaggle-соревнования](https://www.kaggle.com/competitions/cryptonite-hack-sf/).
* По сравнению с baseline-решением мы улучшили результат на 0.09 (baseline-решение показало результат на тесте ~0.54). На момент заполнения README это лучший результат в лидерборде Kaggle-соревнования.
* Продолжается поиск способов улучшения результата.

## Используемые методы
### Предобработка данных
#### Расширение датасета
Цель расширения — повысить робастность модели и сгладить дисбаланс классов в тренировочном датасете.

Для расширения датасета использовали датасет https://huggingface.co/datasets/Djacon/ru-izard-emotions. Из него взяли примеры, содержание метки fear, disgust и sadness (24891 строк). Дополнительно в датасет было добавлено еще ~700 строк **синтезированных (ЧЕМ?)** данных для классов fear и disgust.

* Размер тренировочной выборки, предоставленной заказчиком: 43410 строк.
* Размер тренировочной выборки после расширения датасета: 69973 строк.
* Изменение баланса классов в датасете:
**///**

#### Аугментация данных
Для повышения устойчивости модели к "испорченным" ASR-данным дополнительно применили **(не в датасете, а ОТДЕЛЬНО? ВСТАВИЛИ в текущий топ-ноут?)** к данным аугментацию: 
* Перевод на английский язык и обратно на русский с помощью моделей семейcтва [Helsinki-NLP](https://huggingface.co/Helsinki-NLP)
* Замена части слов случайным синонимом из корпуса wordnet (NLTK).

#### Обработка эмодзи
* (**ОПИСАТЬ РЕЛИЗАЦИЮ**)

#### Обработка ошибок ASR
* (**ОПИСАТЬ РЕЛИЗАЦИЮ**)



### Архитектура модели
Подход, который мы выбрали для решения задачи — это fine-tuning предобученных моделей, размещенных в репозитории Huggingface.

* В рамках исследования мы протестировали ряд BERT-like архитектур моделей (BERT, RoBERTa, DistilBERT и др.), а также некоторые модели с архитектурами других типов.
* Текущий лучший результат показала модель с архитектурой RoBERTA.

### Гиперпараметры
Мы экспериментируем со значениями следующих гиперпараметров:
* Количество эпох
* Величина шага оптимизатора
* Сила регуляризации оптимизатора
* Тип оптимизатора
* Балансировка (weight) в лосс-функции
* Величина порога уверенности для фильтрации предсказаний
* С увеличением числа линейных слоев в «голове» модели мы меняем уровень Dropout, который их разделяет.

### Запуск
Код решения содержится в ноутбуке, представляющем собой модифицированное baseline-решение. Для воспроизведения результатов достаточно выполнить последовательный запуск всех ячеек.

Ноутбук содержит пути до файлов с данными, при запуске их потребуется заменить на актуальные.

### Основные функции в коде
